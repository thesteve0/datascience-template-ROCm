# ROCm GPU Memory Access Fault - Firmware Issue

## Issue Summary
- **Problem**: "Memory access fault by GPU node-1... Page not present or supervisor privilege"
- **GPU**: AMD Radeon 8060S (Strix Halo, gfx1151)
- **Root Cause**: linux-firmware-amdgpu 20251125 update breaks ROCm on AI Max 395
- **Status**: Known bug, was working before firmware update

## Evidence
- Minimal Docker test reproduces issue (not devcontainer config problem):
  ```bash
  docker run --rm -it --device=/dev/kfd --device=/dev/dri --group-add=video \
    rocm/pytorch:rocm7.1_ubuntu24.04_py3.13_pytorch_release_2.9.1 \
    python3 -c "import torch; t = torch.tensor([1.0]).cuda(); print(t)"
  # Result: Memory access fault
  ```

- Current firmware version: linux-firmware-20251125-1.fc43.noarch
- Kernel: 6.17.8-300.fc43.x86_64
- Host ROCm: 6.4.2
- System: Aurora (immutable Fedora)

## Frame.work Community Thread
https://community.frame.work/t/fyi-linux-firmware-amdgpu-20251125-breaks-rocm-on-ai-max-395-8060s/78554

Multiple users confirm same issue on Strix Halo after 20251125 firmware update.

## Fix: Downgrade Firmware (Aurora/Silverblue)

```bash
# On HOST - Downgrade to working version (20251111)
rpm-ostree override replace \
  https://kojipkgs.fedoraproject.org/packages/linux-firmware/20251111/1.fc43/noarch/linux-firmware-20251111-1.fc43.noarch.rpm \
  https://kojipkgs.fedoraproject.org/packages/linux-firmware/20251111/1.fc43/noarch/linux-firmware-amd-gpu-20251111-1.fc43.noarch.rpm \
  https://kojipkgs.fedoraproject.org/packages/linux-firmware/20251111/1.fc43/noarch/linux-firmware-intel-20251111-1.fc43.noarch.rpm \
  https://kojipkgs.fedoraproject.org/packages/linux-firmware/20251111/1.fc43/noarch/linux-firmware-whence-20251111-1.fc43.noarch.rpm

# Enable initramfs
rpm-ostree initramfs --enable

# Reboot
systemctl reboot
```

**After reboot, test:**
```bash
docker run --rm -it --device=/dev/kfd --device=/dev/dri --group-add=video \
  rocm/pytorch:rocm7.1_ubuntu24.04_py3.13_pytorch_release_2.9.1 \
  python3 -c "import torch; t = torch.tensor([1.0]).cuda(); print(t)"
```

## To Undo Downgrade (when fixed firmware arrives):
```bash
rpm-ostree initramfs --disable
rpm-ostree override reset linux-firmware linux-firmware-amd-gpu linux-firmware-intel linux-firmware-whence
systemctl reboot
```

## ~~Questions for Aurora Discord~~ - RESOLVED

Workaround has been implemented. Override will persist until manually reverted.
No further action needed - just wait for upstream linux-firmware fix.

## Template Changes Made (may need reverting)

During debugging, made these changes to devcontainer.json:
1. Added `--group-add=105` (render group) - technically correct per AMD docs
2. Removed `HSA_OVERRIDE_GFX_VERSION: "11.0.0"` - correct for gfx1151

**Decision needed**: Keep these changes (they're AMD-recommended) or revert?

## Current Status
- Devcontainer template is CORRECT - not the issue
- Host firmware is broken
- Workaround implemented: Firmware downgraded to 20251111 using rpm-ostree override
- **Long-term fix**: Waiting for linux-firmware maintainers to release fixed version
  - Once fix is released and merged into Aurora, can revert the override
  - No action needed on our end - just wait for upstream fix to land in Aurora updates

After reboot, verify with:
  rpm -q amd-gpu-firmware
  # Should show: amd-gpu-firmware-20251111-1.fc43.noarch

  Then test the GPU:
  docker run --rm -it --device=/dev/kfd --device=/dev/dri --group-add=video \
    rocm/pytorch:rocm7.1_ubuntu24.04_py3.13_pytorch_release_2.9.1 \
    python3 -c "import torch; t = torch.tensor([1.0]).cuda(); print(t)"


## Reference Links
- Frame.work thread: https://community.frame.work/t/fyi-linux-firmware-amdgpu-20251125-breaks-rocm-on-ai-max-395-8060s/78554
- ROCm Issue #5824: https://github.com/ROCm/ROCm/issues/5824
- AMD ROCm render group requirement: https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/prerequisites.html